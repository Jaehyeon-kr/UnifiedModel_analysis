# Janus

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ë¶„ì„](#ì•„í‚¤í…ì²˜-ë¶„ì„)
3. [Understanding vs Generation ëª¨ë“œ](#understanding-vs-generation-ëª¨ë“œ)
4. [CFGì™€ ë§ˆì§€ë§‰ í† í°](#cfgì™€-ë§ˆì§€ë§‰-í† í°)
5. [Interleaved Generationì˜ ë¶ˆê°€ëŠ¥ì„±](#interleaved-generationì˜-ë¶ˆê°€ëŠ¥ì„±)
6. [ëª¨ë“œ ì„ íƒ ë¬¸ì œ](#ëª¨ë“œ-ì„ íƒ-ë¬¸ì œ)
7. [ê²°ë¡ ](#ê²°ë¡ )

---

## ê°œìš”

JanusëŠ” "Unified Multimodal Understanding and Generation" ëª¨ë¸ë¡œ ì†Œê°œë˜ì§€ë§Œ, ì‹¤ì œ ì•„í‚¤í…ì²˜ë¥¼ ë¶„ì„í•˜ë©´ **Language Modelë§Œ ê³µìœ í•˜ëŠ” Dual-Path êµ¬ì¡°**ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” ì½”ë“œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Janusì˜ ì‹¤ì œ êµ¬ì¡°ì™€ í•œê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

---

## ì•„í‚¤í…ì²˜ ë¶„ì„

### 1. ì‹¤ì œ ê³µìœ  êµ¬ì¡°

**íŒŒì¼**: `janus/models/modeling_vlm.py`

**ë¼ì¸ 190-219**: `MultiModalityCausalLM` í´ë˜ìŠ¤ ì´ˆê¸°í™”

```python
class MultiModalityCausalLM(MultiModalityPreTrainedModel):
    def __init__(self, config: MultiModalityConfig):
        super().__init__(config)

        # Understanding ì „ìš© ì»´í¬ë„ŒíŠ¸
        vision_config = config.vision_config
        vision_cls = model_name_to_cls(vision_config.cls)
        self.vision_model = vision_cls(**vision_config.params)  # CLIP encoder

        aligner_config = config.aligner_config
        aligner_cls = model_name_to_cls(aligner_config.cls)
        self.aligner = aligner_cls(aligner_config.params)  # Image â†’ LLM projection

        # Generation ì „ìš© ì»´í¬ë„ŒíŠ¸
        gen_vision_config = config.gen_vision_config
        gen_vision_cls = model_name_to_cls(gen_vision_config.cls)
        self.gen_vision_model = gen_vision_cls()  # VQ-VAE decoder

        gen_aligner_config = config.gen_aligner_config
        gen_aligner_cls = model_name_to_cls(gen_aligner_config.cls)
        self.gen_aligner = gen_aligner_cls(gen_aligner_config.params)  # Token â†’ LLM

        gen_head_config = config.gen_head_config
        gen_head_cls = model_name_to_cls(gen_head_config.cls)
        self.gen_head = gen_head_cls(gen_head_config.params)  # LLM â†’ Token prediction

        self.gen_embed = torch.nn.Embedding(
            gen_vision_config.params.image_token_size, gen_vision_config.params.n_embed
        )

        # âœ… ìœ ì¼í•˜ê²Œ ê³µìœ ë˜ëŠ” ì»´í¬ë„ŒíŠ¸
        language_config = config.language_config
        self.language_model = LlamaForCausalLM(language_config)
```

### 2. "Unified"ì˜ ì‹¤ì²´

| ì»´í¬ë„ŒíŠ¸ | Understanding | Generation | ê³µìœ  ì—¬ë¶€ |
|---------|---------------|------------|---------|
| Vision Encoder | `self.vision_model` | - | âŒ |
| Vision Decoder | - | `self.gen_vision_model` | âŒ |
| Input Aligner | `self.aligner` | - | âŒ |
| Output Aligner | - | `self.gen_aligner` | âŒ |
| Generation Head | - | `self.gen_head` | âŒ |
| Token Embedding | - | `self.gen_embed` | âŒ |
| **Language Model** | `self.language_model` | `self.language_model` | âœ… |

**ê²°ë¡ **: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ì•½ 70-80%ë¥¼ ì°¨ì§€í•˜ëŠ” Language Modelë§Œ ê³µìœ í•˜ê³ , ë‚˜ë¨¸ì§€ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ëŠ” ì™„ì „íˆ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## Understanding vs Generation ëª¨ë“œ

### 1. Understanding Mode (ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸)

**íŒŒì¼**: `inference.py`

**ë¼ì¸ 36-67**: Understanding ì¶”ë¡  íŒŒì´í”„ë¼ì¸

```python
conversation = [
    {
        "role": "User",
        "content": "<image_placeholder>\nConvert the formula into latex code.",
        "images": ["images/equation.png"],
    },
    {"role": "Assistant", "content": ""},
]

# ì´ë¯¸ì§€ë¥¼ CLIP encoderë¡œ ì¸ì½”ë”©
pil_images = load_pil_images(conversation)
prepare_inputs = vl_chat_processor(
    conversations=conversation, images=pil_images, force_batchify=True
).to(vl_gpt.device)

# Understanding ê²½ë¡œ: vision_model â†’ aligner
inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# Language modelë¡œ í…ìŠ¤íŠ¸ ìƒì„±
outputs = vl_gpt.language_model.generate(
    inputs_embeds=inputs_embeds,
    attention_mask=prepare_inputs.attention_mask,
    max_new_tokens=512,
    do_sample=False,
    use_cache=True,
)
```

**íŒŒì¼**: `janus/models/modeling_vlm.py`

**ë¼ì¸ 221-260**: `prepare_inputs_embeds` ë©”ì„œë“œ

```python
def prepare_inputs_embeds(
    self,
    input_ids: torch.LongTensor,
    pixel_values: torch.FloatTensor,
    images_seq_mask: torch.LongTensor,
    images_emb_mask: torch.LongTensor,
    **kwargs,
):
    bs, n = pixel_values.shape[0:2]
    images = rearrange(pixel_values, "b n c h w -> (b n) c h w")

    # Understanding ì „ìš© ê²½ë¡œ
    images_embeds = self.aligner(self.vision_model(images))  # CLIP â†’ aligner

    images_embeds = rearrange(images_embeds, "(b n) t d -> b (n t) d", b=bs, n=n)
    images_emb_mask = rearrange(images_emb_mask, "b n t -> b (n t)")

    input_ids[input_ids < 0] = 0
    inputs_embeds = self.language_model.get_input_embeddings()(input_ids)

    # ì´ë¯¸ì§€ embeddingì„ í…ìŠ¤íŠ¸ embeddingì— ì‚½ì…
    inputs_embeds[images_seq_mask] = images_embeds[images_emb_mask]

    return inputs_embeds
```

### 2. Generation Mode (í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€)

**íŒŒì¼**: `generation_inference.py`

**ë¼ì¸ 37-52**: Generation í”„ë¡¬í”„íŠ¸ ì¤€ë¹„

```python
conversation = [
    {
        "role": "User",
        "content": "A close-up high-contrast photo of Sydney Opera House...",
    },
    {"role": "Assistant", "content": ""},
]

sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(
    conversations=conversation,
    sft_format=vl_chat_processor.sft_format,
    system_prompt="",
)
# í•µì‹¬: <begin_of_image> í† í° ì¶”ê°€
prompt = sft_format + vl_chat_processor.image_start_tag
```

**ë¼ì¸ 55-108**: Generation ì¶”ë¡  íŒŒì´í”„ë¼ì¸

```python
def generate(
    mmgpt: MultiModalityCausalLM,
    vl_chat_processor: VLChatProcessor,
    prompt: str,
    temperature: float = 1,
    parallel_size: int = 16,
    cfg_weight: float = 5,
    image_token_num_per_image: int = 576,
    img_size: int = 384,
    patch_size: int = 16,
):
    input_ids = vl_chat_processor.tokenizer.encode(prompt)
    input_ids = torch.LongTensor(input_ids)

    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()
    for i in range(parallel_size*2):
        tokens[i, :] = input_ids
        if i % 2 != 0:
            tokens[i, 1:-1] = vl_chat_processor.pad_id  # CFGìš© masking

    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)
    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()

    # 576ê°œ ì´ë¯¸ì§€ í† í° ìˆœì°¨ ìƒì„±
    for i in range(image_token_num_per_image):
        outputs = mmgpt.language_model.model(
            inputs_embeds=inputs_embeds,
            use_cache=True,
            past_key_values=outputs.past_key_values if i != 0 else None
        )
        hidden_states = outputs.last_hidden_state

        # Generation ì „ìš© head ì‚¬ìš©
        logits = mmgpt.gen_head(hidden_states[:, -1, :])
        logit_cond = logits[0::2, :]
        logit_uncond = logits[1::2, :]

        # CFG ì ìš©
        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)
        probs = torch.softmax(logits / temperature, dim=-1)

        next_token = torch.multinomial(probs, num_samples=1)
        generated_tokens[:, i] = next_token.squeeze(dim=-1)

        # ìƒì„±ëœ í† í°ì„ embeddingìœ¼ë¡œ ë³€í™˜ (Generation ì „ìš© ê²½ë¡œ)
        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)
        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)
        inputs_embeds = img_embeds.unsqueeze(dim=1)

    # VQ-VAE decoderë¡œ ì´ë¯¸ì§€ ë³µì›
    dec = mmgpt.gen_vision_model.decode_code(
        generated_tokens.to(dtype=torch.int),
        shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size]
    )
    # ... ì´ë¯¸ì§€ í›„ì²˜ë¦¬
```

**íŒŒì¼**: `janus/models/modeling_vlm.py`

**ë¼ì¸ 262-263**: `prepare_gen_img_embeds` ë©”ì„œë“œ

```python
def prepare_gen_img_embeds(self, image_ids: torch.LongTensor):
    # Generation ì „ìš© ê²½ë¡œ: gen_embed â†’ gen_aligner
    return self.gen_aligner(self.gen_embed(image_ids))
```

### 3. ë‘ ëª¨ë“œì˜ ì™„ì „í•œ ë¶„ë¦¬

| ë‹¨ê³„ | Understanding | Generation |
|-----|--------------|------------|
| **Input** | `<image_placeholder>` + ì‹¤ì œ ì´ë¯¸ì§€ | `<begin_of_image>` í† í° |
| **Encoder** | `vision_model` (CLIP) | - |
| **Projection** | `aligner` | `gen_embed` â†’ `gen_aligner` |
| **LLM** | âœ… `language_model` (ê³µìœ ) | âœ… `language_model` (ê³µìœ ) |
| **Output Head** | LLM head (í…ìŠ¤íŠ¸) | `gen_head` (ì´ë¯¸ì§€ í† í°) |
| **Decoder** | - | `gen_vision_model` (VQ-VAE) |
| **Output** | í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ | ì´ë¯¸ì§€ í”½ì…€ |

---

## CFGì™€ ë§ˆì§€ë§‰ í† í°

### 1. Classifier-Free Guidance (CFG) êµ¬í˜„

**íŒŒì¼**: `generation_inference.py`

**ë¼ì¸ 69-74**: Conditional vs Unconditional ì…ë ¥ ì¤€ë¹„

```python
tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()
for i in range(parallel_size*2):
    tokens[i, :] = input_ids
    if i % 2 != 0:
        # í™€ìˆ˜ ì¸ë±ìŠ¤: ì²« í† í°ê³¼ ë§ˆì§€ë§‰ í† í°ë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” íŒ¨ë”©
        tokens[i, 1:-1] = vl_chat_processor.pad_id
```

**ë¼ì¸ 85-88**: CFG ì ìš©

```python
logits = mmgpt.gen_head(hidden_states[:, -1, :])
logit_cond = logits[0::2, :]      # ì§ìˆ˜ ì¸ë±ìŠ¤: full prompt (conditional)
logit_uncond = logits[1::2, :]    # í™€ìˆ˜ ì¸ë±ìŠ¤: masked prompt (unconditional)

logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)
```

### 2. ë§ˆì§€ë§‰ í† í° ìœ ì§€ì˜ ì˜ë¯¸

**íŒŒì¼**: `janus/models/processing_vlm.py`

**ë¼ì¸ 87-111**: íŠ¹ìˆ˜ í† í° ì •ì˜

```python
def __init__(
    self,
    image_processor: VLMImageProcessor,
    tokenizer: LlamaTokenizerFast,
    image_tag: str = "<image_placeholder>",
    image_start_tag: str = "<begin_of_image>",  # â† ë§ˆì§€ë§‰ í† í°
    image_end_tag: str = "<end_of_image>",
    pad_tag: str = "<ï½œâ–padâ–ï½œ>",
    num_image_tokens: int = 576,
    add_special_token: bool = False,
    sft_format: str = "deepseek",
    mask_prompt: bool = True,
    ignore_id: int = -100,
    **kwargs,
):
    # ...
    self.image_start_tag = image_start_tag
    # ...
```

### 3. ë§ˆì§€ë§‰ í† í°(`<begin_of_image>`)ì„ ìœ ì§€í•˜ëŠ” ì´ìœ 

```
í”„ë¡¬í”„íŠ¸ êµ¬ì¡°: [BOS] [í”„ë¡¬í”„íŠ¸ ë‚´ìš©...] [<begin_of_image>]
                 â†‘       â†‘                    â†‘
               ìœ ì§€    íŒ¨ë”©ìœ¼ë¡œ ê°€ë¦¼          ìœ ì§€

Conditional:   [BOS] [cat, in, garden] [<begin_of_image>]
Unconditional: [BOS] [PAD, PAD, PAD]   [<begin_of_image>]
```

**ì˜ë¯¸**:
- **ì²« í† í° (BOS)**: ë¬¸ì¥ ì‹œì‘ í‘œì‹œ
- **ë§ˆì§€ë§‰ í† í° (`<begin_of_image>`)**: "ì§€ê¸ˆë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì•¼ í•œë‹¤"ëŠ” **êµ¬ì¡°ì  ì‹ í˜¸**
- **ì¤‘ê°„ í”„ë¡¬í”„íŠ¸**: ë¬´ì¡°ê±´ë¶€ ìƒì„±ì„ ìœ„í•´ ë§ˆìŠ¤í‚¹

**íš¨ê³¼**:
- **Conditional**: í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì•Œê³  ì´ë¯¸ì§€ ìƒì„±
- **Unconditional**: "ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì•¼ í•œë‹¤"ëŠ” ê²ƒë§Œ ì•Œê³ , ë‚´ìš©ì€ ëª¨ë¦„
- **CFG**: ë‘ ë¶„í¬ë¥¼ ê²°í•©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì¼ì¹˜ë„ í–¥ìƒ

---

## Interleaved Generationì˜ ë¶ˆê°€ëŠ¥ì„±

### 1. Generation ëª¨ë“œì˜ ê³ ì •ëœ íŒŒì´í”„ë¼ì¸

**íŒŒì¼**: `generation_inference.py`

**ë¼ì¸ 77-95**: 576ê°œ í† í° ê³ ì • ìƒì„±

```python
generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()

# ê³ ì •ëœ ë£¨í”„: ë¬´ì¡°ê±´ 576ê°œ í† í° ìƒì„±
for i in range(image_token_num_per_image):  # image_token_num_per_image = 576
    outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, ...)
    hidden_states = outputs.last_hidden_state

    logits = mmgpt.gen_head(hidden_states[:, -1, :])
    # ...
    next_token = torch.multinomial(probs, num_samples=1)
    generated_tokens[:, i] = next_token.squeeze(dim=-1)

    img_embeds = mmgpt.prepare_gen_img_embeds(next_token)
    inputs_embeds = img_embeds.unsqueeze(dim=1)
```

**ë¬¸ì œì **:
- `<begin_of_image>` í† í° ì´í›„ **ë¬´ì¡°ê±´ 576ê°œ ì´ë¯¸ì§€ í† í° ìƒì„±**
- ì¤‘ê°„ì— í…ìŠ¤íŠ¸ ìƒì„±ìœ¼ë¡œ ì „í™˜ ë¶ˆê°€ëŠ¥
- EOS í† í°ì´ë‚˜ ì¡°ê±´ë¶€ ì¢…ë£Œ ë©”ì»¤ë‹ˆì¦˜ ì—†ìŒ
- í•œ ë²ˆ ì‹œì‘í•˜ë©´ ë°˜ë“œì‹œ ëê¹Œì§€ ì‹¤í–‰

### 2. Understanding ëª¨ë“œì˜ í…ìŠ¤íŠ¸ ì „ìš© ìƒì„±

**íŒŒì¼**: `inference.py`

**ë¼ì¸ 55-64**: LLMì˜ ì¼ë°˜ generate ì‚¬ìš©

```python
outputs = vl_gpt.language_model.generate(
    inputs_embeds=inputs_embeds,
    attention_mask=prepare_inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    max_new_tokens=512,  # í…ìŠ¤íŠ¸ í† í°ë§Œ ìƒì„±
    do_sample=False,
    use_cache=True,
)
```

**ë¬¸ì œì **:
- `language_model.generate()` ì‚¬ìš© â†’ `gen_head`ê°€ í˜¸ì¶œë˜ì§€ ì•ŠìŒ
- ì˜¤ì§ í…ìŠ¤íŠ¸ í† í°ë§Œ ìƒì„± ê°€ëŠ¥
- ì´ë¯¸ì§€ ìƒì„± ë¶ˆê°€ëŠ¥

### 3. ë¶ˆê°€ëŠ¥í•œ ì‚¬ìš© ì¼€ì´ìŠ¤

```
âŒ "ê³ ì–‘ì´ë¥¼ ê·¸ë ¤ì¤˜ [ì´ë¯¸ì§€] ì´ì œ ê°œë„ ê·¸ë ¤ì¤˜ [ì´ë¯¸ì§€]"
   â†’ ë©€í‹°í„´ì—ì„œ ì´ë¯¸ì§€ ìƒì„± ë¶ˆê°€

âŒ "ì„¤ëª…: [í…ìŠ¤íŠ¸] ì˜ˆì‹œ: [ì´ë¯¸ì§€] ì¶”ê°€ ì„¤ëª…: [í…ìŠ¤íŠ¸]"
   â†’ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í˜¼í•© ì¶œë ¥ ë¶ˆê°€

âŒ "ì´ë¯¸ì§€ê°€ í•„ìš”í•˜ë©´ ê·¸ë ¤ì£¼ê³ , ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•´ì¤˜"
   â†’ ì¡°ê±´ë¶€ ëª¨ë‹¬ë¦¬í‹° ì„ íƒ ë¶ˆê°€
```

### 4. ì™œ ë¶ˆê°€ëŠ¥í•œê°€?

**ì•„í‚¤í…ì²˜ì  ì œì•½**:

1. **ë¶„ë¦¬ëœ ì¶œë ¥ í—¤ë“œ**
   - í…ìŠ¤íŠ¸: `language_model.lm_head` (vocab_size ì°¨ì›)
   - ì´ë¯¸ì§€: `gen_head` (image_token_size ì°¨ì›)
   - ë‹¨ì¼ forward passì—ì„œ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥

2. **ë¶„ë¦¬ëœ í† í° ê³µê°„**
   - í…ìŠ¤íŠ¸ í† í°: 0 ~ 32,000 (tokenizer vocabulary)
   - ì´ë¯¸ì§€ í† í°: 0 ~ 8,192 (VQ-VAE codebook)
   - í†µí•©ëœ vocabulary ì—†ìŒ

3. **ë¶„ë¦¬ëœ generation ë¡œì§**
   - Understanding: `language_model.generate()` í˜¸ì¶œ
   - Generation: ì»¤ìŠ¤í…€ ë£¨í”„ + `gen_head` í˜¸ì¶œ
   - ëŸ°íƒ€ì„ì— ì „í™˜ ë¶ˆê°€ëŠ¥

### 5. ë¹„êµ: ì§„ì •í•œ Unified ëª¨ë¸ (Chameleon)

```python
# Chameleonì˜ í†µí•© vocabulary
vocabulary = {
    "text_tokens": 0 ~ 65,536,      # í…ìŠ¤íŠ¸
    "image_tokens": 65,537 ~ 73,728  # ì´ë¯¸ì§€ (8,192ê°œ)
}

# ë‹¨ì¼ autoregressive generation
for i in range(max_length):
    logits = model(input_ids)  # ì „ì²´ vocabularyì— ëŒ€í•œ logits
    next_token = sample(logits)

    if next_token == IMAGE_START:
        # ìì—°ìŠ¤ëŸ½ê²Œ ì´ë¯¸ì§€ í† í° ìƒì„± ì‹œì‘
        continue
    elif next_token == EOS:
        break

    input_ids = torch.cat([input_ids, next_token])
```

---

## ëª¨ë“œ ì„ íƒ ë¬¸ì œ

### 1. í˜„ì¬ í•´ê²°ì±…: ì‚¬ìš©ìê°€ ëª…ì‹œì  ì„ íƒ

**íŒŒì¼**: `demo/app_januspro.py`

**ë¼ì¸ 175-242**: Gradio UI êµ¬ì¡°

```python
# Gradio interface
with gr.Blocks() as demo:
    # Understanding UI
    gr.Markdown(value="# Multimodal Understanding")
    with gr.Row():
        image_input = gr.Image()
        with gr.Column():
            question_input = gr.Textbox(label="Question")
            und_seed_input = gr.Number(label="Seed", precision=0, value=42)
            top_p = gr.Slider(minimum=0, maximum=1, value=0.95, step=0.05, label="top_p")
            temperature = gr.Slider(minimum=0, maximum=1, value=0.1, step=0.05, label="temperature")

    understanding_button = gr.Button("Chat")  # â† Understanding ë²„íŠ¼
    understanding_output = gr.Textbox(label="Response")

    # Generation UI (ë³„ë„ ì„¹ì…˜)
    gr.Markdown(value="# Text-to-Image Generation")

    with gr.Row():
        cfg_weight_input = gr.Slider(minimum=1, maximum=10, value=5, step=0.5, label="CFG Weight")
        t2i_temperature = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.05, label="temperature")

    prompt_input = gr.Textbox(label="Prompt. (Prompt in more detail can help produce better images!)")
    seed_input = gr.Number(label="Seed (Optional)", precision=0, value=12345)

    generation_button = gr.Button("Generate Images")  # â† Generation ë²„íŠ¼
    image_output = gr.Gallery(label="Generated Images", columns=2, rows=2, height=300)

    # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ í´ë¦­
    understanding_button.click(
        multimodal_understanding,
        inputs=[image_input, question_input, und_seed_input, top_p, temperature],
        outputs=understanding_output
    )

    generation_button.click(
        fn=generate_image,
        inputs=[prompt_input, seed_input, cfg_weight_input, t2i_temperature],
        outputs=image_output
    )
```

**ê²°ë¡ **: ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, UIì—ì„œ ë‘ ê°œì˜ ë²„íŠ¼ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

### 2. ë¬¸ì œ ìƒí™©

```python
# ëª¨í˜¸í•œ ìš”ì²­ë“¤
user_input = "ê³ ì–‘ì´ ë³´ì—¬ì¤˜"
# â†’ í…ìŠ¤íŠ¸ ì„¤ëª…? ì´ë¯¸ì§€ ìƒì„±? ğŸ¤”

user_input = "ì•„ë¦„ë‹¤ìš´ í’ê²½"
# â†’ ì„¤ëª…? ìƒì„±? ğŸ¤”

user_input = "ì´ê²ƒ ì¢€ ë§Œë“¤ì–´ì¤˜"
# â†’ ì½”ë“œ? ì´ë¯¸ì§€? ğŸ¤”
```

### 3. ê°€ëŠ¥í•œ í•´ê²°ì±…ë“¤ (ëª¨ë‘ ì¶”ê°€ êµ¬í˜„ í•„ìš”)

#### Option 1: Intent Classifier (ë³„ë„ ëª¨ë¸)

```python
# ì‹¤ì œ ì œí’ˆì—ì„œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì½”ë“œ
def route_request(user_input, has_image):
    # ë³„ë„ì˜ ë¶„ë¥˜ ëª¨ë¸ í•„ìš”
    intent = intent_classifier(user_input)

    keywords_generate = ["ê·¸ë ¤", "ìƒì„±", "ë§Œë“¤ì–´", "draw", "generate", "create"]

    if intent == "understand" and has_image:
        return understanding_mode(user_input, has_image)
    elif intent == "generate" or any(kw in user_input for kw in keywords_generate):
        return generation_mode(user_input)
    else:
        return text_only_mode(user_input)
```

**ë¬¸ì œì **:
- ì¶”ê°€ ëª¨ë¸ í•„ìš” (latency, cost ì¦ê°€)
- ì˜¤ë¶„ë¥˜ ì‹œ ì˜ëª»ëœ ëª¨ë“œ ì‹¤í–‰
- í‚¤ì›Œë“œ ì˜ì¡´ â†’ ì·¨ì•½í•¨

#### Option 2: ëª…ì‹œì  Prefix/Command

```python
# ì‚¬ìš©ìê°€ ì§ì ‘ í‘œì‹œ
"[TEXT] ê³ ì–‘ì´ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"
"[IMAGE] ê³ ì–‘ì´ë¥¼ ê·¸ë ¤ì¤˜"

# ë˜ëŠ” ëª…ë ¹ì–´ ë°©ì‹ (Midjourney)
"/imagine ê³ ì–‘ì´"
"/chat ê³ ì–‘ì´ê°€ ë­ì•¼?"
```

**ë¬¸ì œì **:
- ë‚˜ìœ UX (ì‚¬ìš©ìê°€ ë¬¸ë²• ë°°ì›Œì•¼ í•¨)
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ë¶ˆê°€

#### Option 3: Two-stage Processing

```python
# 1ë‹¨ê³„: ë³„ë„ LLM í˜¸ì¶œë¡œ íŒë‹¨
decision_prompt = f"ì´ ìš”ì²­ì€ ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ì¸ê°€? ì˜ˆ/ì•„ë‹ˆì˜¤\nìš”ì²­: {user_input}"
decision = llm_call(decision_prompt)

# 2ë‹¨ê³„: ì ì ˆí•œ ëª¨ë“œ ì‹¤í–‰
if "ì˜ˆ" in decision:
    return generation_mode(user_input)
else:
    return understanding_mode(user_input)
```

**ë¬¸ì œì **:
- 2ë°° ëŠë¦¼, ë¹„ìš© 2ë°°
- íŒë‹¨ ì˜¤ë¥˜ ê°€ëŠ¥
- ì¶”ê°€ latency

### 4. ë¹„êµ: ì§„ì •í•œ Unified ëª¨ë¸

**Chameleon/GPT-4o ê°™ì€ ëª¨ë¸**:

```python
# ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê²°ì • (ë‹¨ì¼ forward pass)
user: "ê³ ì–‘ì´ë¥¼ ê·¸ë ¤ì¤˜"
model: [TEXT_TOKEN: "ì•Œê² ìŠµë‹ˆë‹¤"]
       [IMAGE_TOKEN_1] [IMAGE_TOKEN_2] ... [IMAGE_TOKEN_576]
       [TEXT_TOKEN: "ì™„ì„±í–ˆì–´ìš”!"]

# ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜
user: "ì´ ì´ë¯¸ì§€ ì„¤ëª…í•´ì¤˜"
model: [TEXT_TOKEN: "ê·€ì—¬ìš´"] [TEXT_TOKEN: "ê³ ì–‘ì´ê°€"] ...

user: "ë¹„ìŠ·í•œ ê±¸ ê·¸ë ¤ì¤˜"
model: [TEXT_TOKEN: "ë„¤"] [IMAGE_TOKEN_1] [IMAGE_TOKEN_2] ...
```

**ì™œ ê°€ëŠ¥?**
- **ë‹¨ì¼ í† í° ê³µê°„**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ í† í°ì´ í•˜ë‚˜ì˜ vocabulary
- **ë‹¨ì¼ ë””ì½”ë”**: í•˜ë‚˜ì˜ transformerê°€ ë‹¤ìŒ í† í° ìœ í˜• ê²°ì •
- **Autoregressive**: ë§¤ ìŠ¤í…ë§ˆë‹¤ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¤‘ ì„ íƒ

### 5. Janusì˜ êµ¬ì¡°ì  ë¶ˆê°€ëŠ¥ì„±

**íŒŒì¼**: `janus/models/modeling_vlm.py` ì „ì²´ êµ¬ì¡°

```python
# í˜„ì¬ êµ¬ì¡°ìƒ ë¶ˆê°€ëŠ¥í•œ ê²ƒë“¤:

âŒ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ mode ì„ íƒ
   â†’ ë‘ ê°œì˜ ë¶„ë¦¬ëœ ê²½ë¡œ, ëŸ°íƒ€ì„ ì „í™˜ ë¶ˆê°€

âŒ ëŒ€í™” ì¤‘ ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜
   â†’ generate() í˜¸ì¶œ ì „ì— ëª¨ë“œ ê³ ì •

âŒ "í•„ìš”í•˜ë©´ ì´ë¯¸ì§€, ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸" ê°™ì€ ìœ ì—°í•œ ì‘ë‹µ
   â†’ ì¶œë ¥ í˜•íƒœê°€ inference ì‹œì‘ ì „ì— ê²°ì •ë¨

âŒ ë‹¨ì¼ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í˜¼í•©
   â†’ ê° ëª¨ë“œê°€ ë‹¨ì¼ modalityë§Œ ì¶œë ¥
```

---

## ê²°ë¡ 

### 1. "Unified"ì˜ ì‹¤ì²´

| ì£¼ì¥ | ì‹¤ì œ |
|-----|------|
| "Unified Multimodal Model" | Language Modelë§Œ ê³µìœ í•˜ëŠ” Dual-Path êµ¬ì¡° |
| "Single Model" | í•˜ë‚˜ì˜ ì²´í¬í¬ì¸íŠ¸ì— ë‘ ê°œì˜ íŒŒì´í”„ë¼ì¸ íŒ¨í‚¹ |
| "Flexible Multimodal" | ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë“œ ì„ íƒ í•„ìš” |

### 2. í•µì‹¬ ì½”ë“œ ê·¼ê±° ìš”ì•½

| ì£¼ì¥ | ê·¼ê±° íŒŒì¼ | ë¼ì¸ | ë‚´ìš© |
|-----|---------|------|------|
| LLMë§Œ ê³µìœ  | `modeling_vlm.py` | 190-219 | Understanding/Generation ì „ìš© ì»´í¬ë„ŒíŠ¸ ë¶„ë¦¬ |
| ëª¨ë“œ ì™„ì „ ë¶„ë¦¬ | `inference.py` | 52-64 | Understanding: `prepare_inputs_embeds` |
| | `generation_inference.py` | 77-108 | Generation: `gen_head` + ì»¤ìŠ¤í…€ ë£¨í”„ |
| CFG ë§ˆì§€ë§‰ í† í° | `generation_inference.py` | 69-74 | `tokens[i, 1:-1] = pad_id` |
| Interleaved ë¶ˆê°€ | `generation_inference.py` | 77 | `for i in range(576)`: ê³ ì • ë£¨í”„ |
| ëª¨ë“œ ì„ íƒ ë¶ˆê°€ | `app_januspro.py` | 232-242 | ë‘ ê°œì˜ ë²„íŠ¼ìœ¼ë¡œ ì‚¬ìš©ì ì„ íƒ |

### 3. ì¥ë‹¨ì 

**ì¥ì ** âœ…:
- ê° taskì— ìµœì í™”ëœ ì„±ëŠ¥ (Understanding, Generation ëª¨ë‘ SOTAê¸‰)
- í•™ìŠµ ì•ˆì •ì„± (ë‘ ëª¨ë“œê°€ ì„œë¡œ ê°„ì„­í•˜ì§€ ì•ŠìŒ)
- ë¹ ë¥¸ ìˆ˜ë ´
- êµ¬í˜„ ë‹¨ìˆœì„±

**ë‹¨ì ** âŒ:
- **Interleaved generation ë¶ˆê°€ëŠ¥**
- **ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ì¶œë ¥ modality ì„ íƒ ë¶ˆê°€**
- íŒŒë¼ë¯¸í„° ë¹„íš¨ìœ¨ (encoder/decoder ì¤‘ë³µ)
- ëª¨ë‹¬ë¦¬í‹° ê°„ ê¹Šì€ ìƒí˜¸ì‘ìš© ë¶€ì¡±
- ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ìœ ì—°ì„± ë¶€ì¡±

### 4. ì§„ì •í•œ Unified vs Janus

| íŠ¹ì„± | Janus | Chameleon/GPT-4o |
|-----|-------|------------------|
| í† í° ê³µê°„ | ë¶„ë¦¬ (í…ìŠ¤íŠ¸ / ì´ë¯¸ì§€) | í†µí•© |
| ë””ì½”ë” | ì´ì¤‘ ê²½ë¡œ | ë‹¨ì¼ ê²½ë¡œ |
| ì¶œë ¥ í—¤ë“œ | ë¶„ë¦¬ (`lm_head` / `gen_head`) | í†µí•© |
| ëª¨ë“œ ì„ íƒ | ì‚¬ìš©ì/ì™¸ë¶€ ì‹œìŠ¤í…œ | ëª¨ë¸ ìì²´ |
| Interleaved | âŒ | âœ… |
| êµ¬í˜„ ë‚œì´ë„ | ë‚®ìŒ | ë†’ìŒ |
| ì„±ëŠ¥ ìµœì í™” | ì‰¬ì›€ | ì–´ë ¤ì›€ |

### 5. ìµœì¢… í‰ê°€

JanusëŠ”:
```
"Language Modelì„ ê³µìœ í•˜ëŠ” ë‘ ê°œì˜ ë…ë¦½ì  ëª¨ë¸"
â‰  "ì§„ì •í•œ Unified Multimodal Model"
```

**ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•œ í‘œí˜„**:
- âœ… "Dual-Path Multimodal Model with Shared LLM Backbone"
- âœ… "Multitask Multimodal Model with Unified Language Representation"
- âŒ "Unified Multimodal Understanding and Generation Model"

**ì‹¤ìš©ì  ì˜ë¯¸**:
- ì—°êµ¬/ë²¤ì¹˜ë§ˆí¬: ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥
- ì‹¤ì œ ì œí’ˆ: ì¶”ê°€ ë¼ìš°íŒ… ë¡œì§ í•„ìš”
- ìœ ì—°í•œ ëŒ€í™”í˜• AI: ê·¼ë³¸ì  í•œê³„ ì¡´ì¬

---

## ì°¸ê³  ìë£Œ

### ì£¼ìš” ë¶„ì„ íŒŒì¼
- `janus/models/modeling_vlm.py`: í•µì‹¬ ì•„í‚¤í…ì²˜
- `generation_inference.py`: Generation ëª¨ë“œ ì¶”ë¡ 
- `inference.py`: Understanding ëª¨ë“œ ì¶”ë¡ 
- `demo/app_januspro.py`: ì‹¤ì œ ë°°í¬ ì˜ˆì‹œ
- `janus/models/processing_vlm.py`: í† í° ì²˜ë¦¬ ë¡œì§

### ë¹„êµ ëŒ€ìƒ ëª¨ë¸
- **Chameleon**: ì§„ì •í•œ unified token space
- **Emu2**: Unified autoregressive framework
- **Transfusion**: Diffusion + LLM í•˜ì´ë¸Œë¦¬ë“œ
- **SEED-X**: Unified embedding space

---

**ì‘ì„± ì¼ì**: 2025-11-03
**ë¶„ì„ ëŒ€ìƒ**: Janus-1.3B / Janus-Pro-7B ì½”ë“œë² ì´ìŠ¤
