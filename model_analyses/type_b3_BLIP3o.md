# BLIP3o-NEXT Model Analysis

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ë¶„ì„](#ì•„í‚¤í…ì²˜-ë¶„ì„)
3. [í† í° ì‹œìŠ¤í…œ](#í† í°-ì‹œìŠ¤í…œ)
4. [BOI Token ë©”ì»¤ë‹ˆì¦˜](#boi-token-ë©”ì»¤ë‹ˆì¦˜)
5. [Interleaved Generation](#interleaved-generation)
6. [í•µì‹¬ íŠ¹ì§•](#í•µì‹¬-íŠ¹ì§•)
7. [SEED-Xì™€ì˜ ë¹„êµ](#seed-xì™€ì˜-ë¹„êµ)
8. [ê²°ë¡ ](#ê²°ë¡ )

---

## ê°œìš”

**BLIP3o-NEXT**ëŠ” **AR (Autoregressive) + Diffusion**ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì…ë‹ˆë‹¤.

### ê¸°ë³¸ ì •ë³´
- **íƒ€ì…**: Type B3 (Learnable Query) + Diffusion Hybrid
- **ë² ì´ìŠ¤ LLM**: Qwen3-3B
- **Visual Encoder**: TATok (Text-Aligned Tokenizer) with SigLIP-2
- **Image Generator**: Sana (Diffusion Transformer)
- **ì´ë¯¸ì§€ í† í° ìˆ˜**: 256 tokens (16x16 grid)
- **ë…¼ë¬¸**: [BLIP3o-NEXT arxiv](http://arxiv.org/abs/2510.15857)

### ëª¨ë¸ íŠ¹ì§•
```
Understanding Path:
Input Image â†’ TATok â†’ Discrete Tokens (256) â†’ LLM Embeddings â†’ Text Response

Generation Path:
Text Prompt â†’ AR Model â†’ Discrete Token Sequence (256)
           â†’ Hidden States â†’ Diffusion Connector
           â†’ Sana Diffusion â†’ VAE Decoder â†’ Output Image
```

---

## ì•„í‚¤í…ì²˜ ë¶„ì„

### 1. ì „ì²´ êµ¬ì¡° (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\model\blip3o_arch.py)

```python
class blip3oMetaModel:
    def __init__(self, config):
        # Vision Tower (Understandingìš©)
        self.vision_tower = build_vision_tower(config, delay_load=delay_load)

        # Diffusion Model (Generationìš©)
        self.sana = build_sana(config)  # SanaTransformer2DModel
        self.sana_vae = build_vae(config)  # AutoencoderDC

        # AR â†’ Diffusion ë¸Œë¦¿ì§€
        self.diffusion_connector = nn.Sequential(
            nn.Linear(config.hidden_size, 2304),
            nn.GELU(approximate="tanh"),
            nn.Linear(2304, 2304),
            RMSNorm(2304, eps=1e-5),
        )

        # Noise Scheduler
        self.noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(...)
```

**í•µì‹¬**: AR ëª¨ë¸ì˜ hidden statesë¥¼ diffusion conditionìœ¼ë¡œ ë³€í™˜í•˜ëŠ” `diffusion_connector`

---

### 2. TATok: Text-Aligned Tokenizer

#### 2.1 êµ¬ì¡° (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\tok\ta_tok.py)

```python
class TextAlignedTokenizer(nn.Module):
    def __init__(
        self,
        bottleneck_token_num=256,  # ê³ ì • 256 í† í°
        input_size=384,
        teacher='google/siglip2-so400m-patch14-384',
        input_type='rec',  # 'quant', 'rec', 'indices'
    ):
        # SigLIP-2 Encoder
        self.encoder = AutoModel.from_config(self.encoder_config).vision_model

        # Decoder (feature reconstructionìš©)
        self.decoder = Siglip2VisionModel(self.decoder_config)

        # Bottleneck (VQ layer)
        self.bottleneck = models.make(bottleneck, args={
            'token_nums': self.bottleneck_token_num,
            'input_dim': self.encoder_hidden_dim,
            'output_dim': self.bottleneck_dim
        })
```

**íŠ¹ì§•**:
- SigLIP-2 ê¸°ë°˜ì˜ discrete visual tokenizer
- 256ê°œì˜ learnable query tokens (SEED-Xì˜ 64ê°œë³´ë‹¤ ë§ìŒ)
- VQ (Vector Quantization) ê¸°ë°˜

---

#### 2.2 ì¸ì½”ë”© ê³¼ì •

```python
def encode(self, x, **kwargs):
    # 1. SigLIP-2ë¡œ visual features ì¶”ì¶œ
    vq_feats = self.encoder(x, output_hidden_states=True).hidden_states[-2]

    # 2. Optional pooling
    if pool_scale != 1:
        vq_feats = self.avg_pool(vq_feats, pool_scale)

    # 3. Task-specific projection
    vq_feats = self.encode_task_layer(vq_feats)

    # 4. Bottleneck (VQ)
    bottleneck_out = self.bottleneck(vq_feats)

    return {
        'encoded': z,                    # Quantized features
        'vq_feats': vq_feats,           # Original features
        'bottleneck_rep': indices,       # Discrete indices
    }
```

**VQ ë ˆì´ì–´** (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\tok\ar_dtok\bottleneck.py:70-163):

```python
class SimVectorQuantizer(nn.Module):
    def __init__(self, dim, codebook_size, l2_normalized=False):
        self.codebook_size = codebook_size  # ì˜ˆ: 8192
        self.embedding = nn.Embedding(codebook_size, dim)
        self.embedding_proj = nn.Linear(dim, dim)

    def forward(self, z):
        # L2 normalization (ì„ íƒì )
        if self.l2_normalized:
            z = F.normalize(z, p=2, dim=-1)

        # Codebook lookup
        d = torch.sum(z**2, dim=1, keepdim=True) + torch.sum(emb**2, dim=1) \
            - 2 * torch.einsum("bd,dn->bn", z_flattened, emb.t())
        q_indices = torch.argmin(d, dim=1)

        # Quantization with straight-through estimator
        quantized = F.embedding(q_indices, emb).view(z.shape)
        quantized = z + (quantized - z).detach()

        return {
            'regularized_z': quantized,      # For forward pass
            'bottleneck_rep': q_indices      # Discrete indices
        }
```

**í•µì‹¬**: Discrete indicesê°€ LLMì˜ vocabularyì— ì¶”ê°€ë¨

---

### 3. LLM Integration

#### 3.1 ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\model\blip3o_arch.py:155-165)

```python
def encode_images(self, images, modalities, pool_scale=None):
    # TATokìœ¼ë¡œ ì´ë¯¸ì§€ ì¸ì½”ë”©
    image_features = self.get_model().get_vision_tower()(images, pool_scale=pool_scale)

    # Discrete tokens ì¶”ì¶œ
    image_tokens = image_features['tokens']  # [B, 256] indices

    # Discrete tokensë¥¼ LLM vocabulary rangeë¡œ shift
    image_tokens = image_tokens + self.config.image_start_token_id

    # LLMì˜ embedding layerë¡œ ë³€í™˜
    image_features = self.get_model().embed_tokens(image_tokens)

    return {'image_features': image_features, 'image_tokens': image_tokens}
```

**í•µì‹¬ ì°¨ì´ì **:
- SEED-X: Resamplerë¡œ continuous features ìƒì„±
- BLIP3o: Discrete tokensë¥¼ ì§ì ‘ LLM vocabularyì— ì¶”ê°€

---

#### 3.2 Vocabulary Extension (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\model\blip3o_arch.py:375-400)

```python
def initialize_vision_tokenizer(self, model_args, tokenizer):
    # 1. ìŠ¤ì¼€ì¼ í† í° ì¶”ê°€ (multi-scale support)
    if model_args.num_scale_tokens > 0:
        scale_tokens = [f"<S{i}>" for i in range(num_scale_tokens)]
        tokenizer.add_tokens(scale_tokens, special_tokens=False)
        self.config.scale_start_token_id = ...

    # 2. ì´ë¯¸ì§€ í† í° ì¶”ê°€ (discrete visual tokens)
    if model_args.num_image_tokens > 0:
        image_tokens = [f"<IMG_{i}>" for i in range(num_image_tokens)]
        tokenizer.add_tokens(image_tokens, special_tokens=False)
        self.config.image_start_token_id = ...
        self.config.num_image_tokens = num_image_tokens

    # 3. Vision embeddingsë¡œ ì´ˆê¸°í™” (ì„ íƒì )
    if model_args.load_embeddings_from_vision:
        vision_embeddings = vision_tower.get_embedding()
        input_embeddings[
            self.config.image_start_token_id:
            self.config.image_end_token_id+1
        ] = vision_embeddings
```

**ì„¤ëª…**:
- `num_image_tokens`ê°œì˜ discrete tokensë¥¼ LLM vocabularyì— ì¶”ê°€
- TATokì˜ codebook embeddingsë¡œ ì´ˆê¸°í™” ê°€ëŠ¥

---

## í† í° ì‹œìŠ¤í…œ

### 1. íŠ¹ìˆ˜ í† í° (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\constants.py)

```python
IGNORE_INDEX = -100
IMAGE_TOKEN_INDEX = -200
DEFAULT_IMAGE_TOKEN = "<image>"      # Placeholder
DEFAULT_IM_START_TOKEN = "<im_start>"  # ì´ë¯¸ì§€ ì‹œì‘
DEFAULT_IM_END_TOKEN = "<im_end>"      # ì´ë¯¸ì§€ ë
```

**ì¶”ê°€ í† í°ë“¤**:
- `<S0>`, `<S1>`, `<S2>`, `<S3>`: Multi-scale tokens (í•´ìƒë„ ì§€ì •)
- `<IMG_0>` ~ `<IMG_N>`: Discrete visual tokens (codebook sizeë§Œí¼)

---

### 2. ì‹œí€€ìŠ¤ êµ¬ì¡°

#### Understanding ì‹œí€€ìŠ¤:
```
[User message]<image>[/User]
[Assistant]<im_start><S1>[IMG_1234][IMG_5678]...[IMG_7890]<im_end>[text response][/Assistant]
                     ^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                     scale  256 discrete visual tokens
```

#### Generation ì‹œí€€ìŠ¤ (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\inference.py:35-43):
```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": f"Please generate image based on: {prompt}"}
]
input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
input_text += f"<im_start><S{scale}>"  # BOI í† í° ìˆ˜ë™ ì¶”ê°€
                      ^^^
                      scale=0: 1024x1024 í•´ìƒë„
```

**í•µì‹¬**: `<im_start>`ì™€ scale tokenì´ **ì½”ë“œì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€**ë¨

---

## BOI Token ë©”ì»¤ë‹ˆì¦˜

### ê²°ë¡ : âŒ ëª¨ë¸ì´ ìë™ ì˜ˆì¸¡í•˜ì§€ ì•ŠìŒ

### ì¦ê±° 1: ì¶”ë¡  ì½”ë“œ (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\inference.py:30-56)

```python
def generate_image(self, prompt: str) -> Image.Image:
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Please generate image based on: {prompt}"}
    ]

    # Chat template ì ìš©
    input_text = self.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # âš ï¸ BOI í† í° ìˆ˜ë™ ì¶”ê°€
    input_text += f"<im_start><S{self.config.scale}>"
    #              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    #              í•­ìƒ ì½”ë“œì—ì„œ ì¶”ê°€ë¨

    inputs = self.tokenizer(input_text, return_tensors="pt")

    # AR generation
    gen_ids, output_image = self.model.generate_images(
        inputs.input_ids,
        inputs.attention_mask,
        max_new_tokens=self.config.seq_len,  # 729 tokens
        ...
    )
```

**ì„¤ëª…**:
- `<im_start>`ì™€ `<S{scale}>` í† í°ì´ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— **í•­ìƒ ì¶”ê°€**ë¨
- ëª¨ë¸ì€ ì´í›„ 256ê°œì˜ discrete image tokensë§Œ ìƒì„±

---

### ì¦ê±° 2: Generation ë©”ì„œë“œ (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\model\language_model\blip3o_qwen_inference.py:122-230)

```python
@torch.no_grad()
def generate_images(
    self,
    inputs,  # ì´ë¯¸ <im_start><S0>ê°€ í¬í•¨ëœ ìƒíƒœ
    attention_mask,
    max_new_tokens=729,  # 729 = 1 (scale) + 256 (image) + padding
    ...
):
    # 1ë‹¨ê³„: AR ëª¨ë¸ë¡œ discrete tokens ìƒì„±
    gen_ids = super().generate(
        inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k
    )
    # gen_ids: [<im_start><S0><IMG_1234><IMG_5678>...<im_end>]

    # 2ë‹¨ê³„: Hidden states ì¶”ì¶œ
    with torch.no_grad():
        outs = self.model(
            input_ids=gen_ids,
            output_hidden_states=True,
            return_dict=True,
        )
    hidden_states = outs.hidden_states[-1]  # [B, Seq, Hidden]

    # 3ë‹¨ê³„: <im_start>ì™€ <im_end> ì‚¬ì´ì˜ hidden states ì¶”ì¶œ
    start_pos = (gen_ids == self.config.image_start_tag_id).argmax(dim=1)
    end_pos = (gen_ids == self.config.image_end_tag_id).argmax(dim=1)

    selected_hidden_states = []
    for b in range(hidden_states.size(0)):
        start = start_pos[b].item() + 1  # <im_start> ë‹¤ìŒë¶€í„°
        selected_hidden_states.append(hidden_states[b, start:, :])
    pred_latent = torch.stack(selected_hidden_states, dim=0)  # [B, 256, Hidden]

    # 4ë‹¨ê³„: Diffusion generation
    img_hidden_states_null = torch.zeros_like(pred_latent)  # CFGìš©
    pred_latent = torch.cat([img_hidden_states_null, pred_latent], 0)

    # Latent ì´ˆê¸°í™”
    latents = randn_tensor(
        shape=(bsz, latent_channels, 32, 32),  # 32x32 latent
        generator=None,
        device=device,
        dtype=torch.bfloat16,
    )

    # Diffusion loop
    for t in tqdm(self.noise_scheduler.timesteps):
        latent_model_input = torch.cat([latents] * 2)  # CFG

        # Sana Diffusion Transformer
        noise_pred = self.sana(
            hidden_states=latent_model_input,
            encoder_hidden_states=self.diffusion_connector(pred_latent),
            #                      ^^^^^^^^^^^^^^^^^^^^^^^^
            #                      ARì˜ hidden statesë¥¼ conditionìœ¼ë¡œ
            timestep=t,
            encoder_attention_mask=None
        ).sample

        # Classifier-Free Guidance
        noise_pred_uncond, noise_pred = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred - noise_pred_uncond)

        # Denoising step
        latents = self.noise_scheduler.step(noise_pred, t, latents).prev_sample

    # 5ë‹¨ê³„: VAE decoding
    samples = self.decode_latents(latents)

    return gen_ids, samples
```

**í•µì‹¬**:
1. **AR ë‹¨ê³„**: `<im_start><S0>` ì´í›„ 256ê°œì˜ discrete tokens ìƒì„±
2. **Hidden state ì¶”ì¶œ**: AR ëª¨ë¸ì˜ ë§ˆì§€ë§‰ layer hidden states
3. **Diffusion condition**: Hidden statesë¥¼ `diffusion_connector`ë¡œ ë³€í™˜
4. **Diffusion ë‹¨ê³„**: Sana Transformerë¡œ ì´ë¯¸ì§€ ìƒì„±

---

### SEED-Xì™€ì˜ ì°¨ì´ì 

| í•­ëª© | SEED-X | BLIP3o-NEXT |
|------|---------|-------------|
| **AR ì¶œë ¥** | 64ê°œ ê³ ì • í† í° (deterministic) | 256ê°œ discrete tokens (stochastic) |
| **Diffusion** | SDXL-Turbo (UNet) | Sana (DiT) |
| **Condition** | De-tokenized continuous features | AR hidden states (ì§ì ‘) |
| **Vocab í™•ì¥** | 64 tokens (`<IMG_0>` ~ `<IMG_63>`) | ìˆ˜ì²œ~ìˆ˜ë§Œ tokens (codebook size) |

---

## Interleaved Generation

### ê²°ë¡ : âŒ ì™„ì „ ìë™í™”ëœ interleaved generation ë¶ˆê°€

### ì´ìœ 

#### 1. ë‘ ë‹¨ê³„ ë¶„ë¦¬ ì•„í‚¤í…ì²˜

**AR ë‹¨ê³„**:
```python
gen_ids = super().generate(inputs, max_new_tokens=729, ...)
# ì¶œë ¥: discrete token sequenceë§Œ ìƒì„±
```

**Diffusion ë‹¨ê³„**:
```python
for t in self.noise_scheduler.timesteps:
    noise_pred = self.sana(
        hidden_states=latent_model_input,
        encoder_hidden_states=self.diffusion_connector(pred_latent),
        timestep=t,
    ).sample
    latents = self.scheduler.step(noise_pred, t, latents).prev_sample

samples = self.decode_latents(latents)  # ìµœì¢… ì´ë¯¸ì§€
```

**ë¬¸ì œì **: AR ë‹¨ê³„ì™€ Diffusion ë‹¨ê³„ê°€ **ì™„ì „íˆ ë¶„ë¦¬**ë˜ì–´ ìˆìŒ

---

#### 2. generate_images vs generate ë©”ì„œë“œ ë¶„ë¦¬

```python
# Text generation (ì´í•´)
generated_text = model.generate(
    inputs=input_ids,
    images=images,
    max_new_tokens=512,
    ...
)

# Image generation (ìƒì„±)
gen_ids, generated_images = model.generate_images(
    inputs=input_ids,
    max_new_tokens=729,
    guidance_scale=2.0,
    num_inference_steps=30,
    ...
)
```

**ì„¤ëª…**:
- `generate()`: Text-only generation (ì´í•´ íƒœìŠ¤í¬ìš©)
- `generate_images()`: AR + Diffusion pipeline (ìƒì„± íƒœìŠ¤í¬ìš©)
- ë‘ ë©”ì„œë“œê°€ **ì™„ì „íˆ ë¶„ë¦¬**ë¨

---

#### 3. ì§„ì •í•œ Interleavedë¥¼ ìœ„í•œ ìš”êµ¬ì‚¬í•­

Chameleon ìŠ¤íƒ€ì¼ì˜ interleaved generation:
```
User: "Describe this image and create a similar one."
Model: "This is a cat. <generated_image_1> Here's a similar cat: <generated_image_2>"
                        ^^^^^^^^^^^^^^^^^^^                      ^^^^^^^^^^^^^^^^^^^
                        AR + Diffusion                          AR + Diffusion
```

**BLIP3o-NEXTì˜ í•œê³„**:
1. `generate_images()`ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•´ì•¼ í•¨
2. í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ diffusionì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŒ
3. ë‹¨ì¼ forward passë¡œ text + image ë™ì‹œ ìƒì„± ë¶ˆê°€

---

## í•µì‹¬ íŠ¹ì§•

### 1. Discrete Image Token Supervision

**Training Objective** (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\model\language_model\blip3o_qwen.py:104-166):

```python
def forward(self, input_ids, labels, images, target_images, ...):
    # 1. LLM forward pass
    outputs = self.model(input_ids=input_ids, ...)
    hidden_states = outputs[0]
    logits = self.lm_head(hidden_states)

    # 2. Cross-Entropy loss (discrete tokens ì˜ˆì¸¡)
    if labels is not None:
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss = CrossEntropyLoss()(shift_logits, shift_labels)

    # 3. Diffusion loss (ì´ë¯¸ì§€ í’ˆì§ˆ)
    if target_images is not None:
        # VAE encoding
        vae = self.model.get_sana_vae()
        latents = vae.encode(target_images).latent * vae.config.scaling_factor

        # Add noise
        noise = torch.randn_like(latents)
        timesteps = sample_timesteps(batch_size)
        noisy_latents = (1.0 - sigmas) * latents + sigmas * noise

        # Extract hidden states for image region
        start_pos = (labels == self.config.image_start_tag_id).argmax(dim=1)
        end_pos = (labels == self.config.image_end_tag_id).argmax(dim=1)
        selected_hidden_states = hidden_states[b, start:end, :]

        # Diffusion prediction
        diffusion_pred = self.sana(
            hidden_states=noisy_latents,
            timestep=timesteps,
            encoder_hidden_states=self.diffusion_connector(selected_hidden_states),
        ).sample

        # Flow matching loss
        target = noise - latents
        diff_loss = ((diffusion_pred - target) ** 2).mean()

        # Combined loss
        loss += diff_loss

    return loss
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
1. **AR ëª¨ë¸**: Discrete image tokensë¥¼ ì˜ˆì¸¡ (CrossEntropy loss)
2. **Diffusion ëª¨ë¸**: ARì˜ hidden statesë¥¼ conditionìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± (MSE loss)
3. **Joint training**: ë‘ lossë¥¼ ë™ì‹œì— ìµœì í™”

**ì¥ì **:
- Discrete tokensê°€ "blueprint" ì—­í• 
- Structural accuracy (AR) + Visual fidelity (Diffusion)

---

### 2. RL with GRPO

**ì™œ ê°€ëŠ¥í•œê°€?**

Discrete tokens ë•ë¶„ì— RL í”„ë ˆì„ì›Œí¬ ì‚¬ìš© ê°€ëŠ¥:

```python
# Reward modelë¡œ ìƒì„±ëœ ì´ë¯¸ì§€ í‰ê°€
reward = reward_model(generated_image, prompt)

# Token-level policy gradient
policy_loss = -reward * log_prob(token_sequence)
```

**GRPO (Group Relative Policy Optimization)**:
- GenEval, T2I-Compbenchì—ì„œ í”„ë¡¬í”„íŠ¸ ì •ë ¬ ë° í…ìŠ¤íŠ¸ ë Œë”ë§ ê°œì„ 
- Discrete tokensì— ëŒ€í•œ policy gradient

**ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµ**:
- **Show-o**: Discrete diffusion (MaskGIT) - RL ê°€ëŠ¥
- **UniToken**: Continuous representations - RL ë¶ˆê°€
- **BLIP3o**: Discrete AR tokens - RL ê°€ëŠ¥

---

### 3. Multi-Scale Support

**Scale Tokens** (d:\Check_\janus\analysis\repos\type_b3_learnable_query\BLIP3o\blip3o\model\multimodal_encoder\ta_tok_encoder.py:68-83):

```python
def forward(self, images, pool_scale=1):
    # SigLIP-2 encoding
    vq_feats = self.vision_tower(images, output_hidden_states=True).hidden_states[-2]

    # Multi-scale pooling
    if pool_scale != 1:
        vq_feats = self.avg_pool(vq_feats, pool_scale)
        # pool_scale=1: 16x16 = 256 tokens
        # pool_scale=2: 8x8 = 64 tokens
        # pool_scale=3: ~5x5 = 32 tokens (approximate)

    # VQ encoding
    bottleneck_out = self.bottleneck(vq_feats)
    tokens = bottleneck_out['bottleneck_rep']  # Discrete indices

    return {"tokens": tokens, 'pool_scale': pool_scale}
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# High resolution
input_text += "<im_start><S0>"  # scale=0 â†’ 1024x1024

# Low resolution (faster)
input_text += "<im_start><S1>"  # scale=1 â†’ 512x512
```

**Training**:
```python
if self.training:
    pool_scale = random.choice(vision_tower.pool_scales)  # [1, 2, 3]
else:
    pool_scale = 1  # Always full resolution for evaluation
```

---

## SEED-Xì™€ì˜ ë¹„êµ

### ì•„í‚¤í…ì²˜ ì°¨ì´

| ì¸¡ë©´ | SEED-X | BLIP3o-NEXT |
|------|--------|-------------|
| **íƒ€ì…** | Type B3 (Learnable Query) | Type B3 + Diffusion Hybrid |
| **Visual Encoder** | Q-Former + Resampler | TATok (SigLIP-2 + VQ) |
| **í† í° ìˆ˜** | 64 (ê³ ì •) | 256 (ê³ ì •) |
| **í† í° íƒ€ì…** | Learnable queries | Discrete codebook indices |
| **LLM Integration** | Continuous embeddings | Discrete tokens in vocabulary |
| **Image Generator** | SDXL-Turbo (UNet) | Sana (DiT - Flow Matching) |
| **Diffusion Condition** | De-tokenized features | AR hidden states |
| **Generation ë°©ì‹** | AR â†’ De-tokenizer â†’ Diffusion | AR â†’ Hidden states â†’ Diffusion |

---

### BOI Token

| ëª¨ë¸ | BOI ì˜ˆì¸¡ | ë©”ì»¤ë‹ˆì¦˜ |
|------|----------|----------|
| **SEED-X** | âŒ | `AutoImageTokenGenerationProcessor`ê°€ ê°•ì œ ì‚½ì… |
| **BLIP3o-NEXT** | âŒ | ì½”ë“œì—ì„œ `<im_start><S{scale}>` ìˆ˜ë™ ì¶”ê°€ |

**ê³µí†µì **: ë‘˜ ë‹¤ BOI í† í°ì„ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ì§€ ì•ŠìŒ

---

### Interleaved Generation

| ëª¨ë¸ | Interleaved | ë©”ì»¤ë‹ˆì¦˜ |
|------|-------------|----------|
| **SEED-X** | âš ï¸ ì œí•œì  | ë‘ ë²ˆì˜ ì¶”ë¡  í•„ìš” (AR + Diffusion) |
| **BLIP3o-NEXT** | âŒ | `generate()`ì™€ `generate_images()` ì™„ì „ ë¶„ë¦¬ |

**ê³µí†µì **: ë‘˜ ë‹¤ ì™„ì „ ìë™í™”ëœ interleaved generation ë¶ˆê°€

---

### í† í° ì‹œí€€ìŠ¤ ë¹„êµ

**SEED-X**:
```
<|begin_of_image|><IMG_0><IMG_1>...<IMG_63><|end_of_image|>
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                  64ê°œ ê³ ì •, í•­ìƒ ë™ì¼í•œ ìˆœì„œ (deterministic)
```

**BLIP3o-NEXT**:
```
<im_start><S0><IMG_1234><IMG_5678>...<IMG_7890><im_end>
          ^^^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          scale  256ê°œ, ê° ìœ„ì¹˜ë§ˆë‹¤ codebookì—ì„œ ì„ íƒ (stochastic)
```

---

### Training Objectives

**SEED-X**:
```python
# 1. Deterministic sequence loss
loss = CrossEntropyLoss(logits, fixed_sequence)  # <IMG_0> ~ <IMG_63>

# 2. Diffusion loss (SDXL-Turbo)
loss += MSE(predicted_noise, target_noise)
```

**BLIP3o-NEXT**:
```python
# 1. Discrete token prediction loss
loss = CrossEntropyLoss(logits, discrete_tokens)  # Stochastic

# 2. Diffusion loss (Sana - Flow Matching)
loss += MSE(diffusion_pred, noise - latents)
```

**í•µì‹¬ ì°¨ì´**:
- SEED-X: ê³ ì •ëœ 64ê°œ ì‹œí€€ìŠ¤ â†’ ë‹¤ì–‘ì„± ì œí•œ
- BLIP3o: Codebookì—ì„œ ììœ ë¡­ê²Œ ì„ íƒ â†’ ë†’ì€ ë‹¤ì–‘ì„±

---

## ê²°ë¡ 

### BOI Token ì˜ˆì¸¡
**âŒ ë¶ˆê°€ëŠ¥**

- `<im_start><S{scale}>` í† í°ì´ **ì½”ë“œì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€**ë¨
- ëª¨ë¸ì€ ì´ë¯¸ì§€ ì‹œì‘ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ì§€ ëª»í•¨

---

### Interleaved Generation
**âŒ ì™„ì „ ìë™í™” ë¶ˆê°€**

**ì´ìœ **:
1. AR ë‹¨ê³„ì™€ Diffusion ë‹¨ê³„ê°€ **ì™„ì „íˆ ë¶„ë¦¬**
2. `generate()`ì™€ `generate_images()` ë©”ì„œë“œ ë¶„ë¦¬
3. í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ diffusion ì‹¤í–‰ ë¶ˆê°€

**ê°€ëŠ¥í•œ ê²ƒ**:
- ëª…ì‹œì ìœ¼ë¡œ `generate_images()` í˜¸ì¶œí•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±
- ìˆ˜ë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ êµì°¨ ë°°ì¹˜

**ë¶ˆê°€ëŠ¥í•œ ê²ƒ**:
- Chameleon ìŠ¤íƒ€ì¼ì˜ single forward pass interleaved generation
- ëª¨ë¸ì´ ììœ¨ì ìœ¼ë¡œ "ì§€ê¸ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì•¼ í•¨"ì„ ê²°ì •

---

### BLIP3o-NEXTì˜ ë…íŠ¹í•œ ì 

#### 1. **Discrete Image Token Supervision**
- AR ëª¨ë¸ì´ discrete tokensë¥¼ ì˜ˆì¸¡
- "Blueprint" ì—­í• ë¡œ structural accuracy ì œê³µ
- Diffusionì´ visual fidelity ë‹´ë‹¹

#### 2. **RL Compatibility**
- Discrete tokens ë•ë¶„ì— GRPO ê°™ì€ RL ê°€ëŠ¥
- Text-to-image alignment ë° text rendering ê°œì„ 

#### 3. **AR + Diffusion Hybrid**
- ARì˜ êµ¬ì¡°ì  ì •í™•ì„±
- Diffusionì˜ ì‹œê°ì  í’ˆì§ˆ
- ë‘ ê°€ì§€ ì¥ì  ê²°í•©

---

### ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµ

| íŠ¹ì§• | Chameleon | SEED-X | BLIP3o-NEXT |
|------|-----------|--------|-------------|
| **BOI ì˜ˆì¸¡** | âœ… ê°€ëŠ¥ | âŒ ë¶ˆê°€ | âŒ ë¶ˆê°€ |
| **Interleaved** | âœ… ì™„ì „ ìë™ | âš ï¸ ì œí•œì  | âŒ ë¶ˆê°€ |
| **Token íƒ€ì…** | Discrete (VQ-VAE) | Learnable queries | Discrete (VQ) + Continuous (Diffusion) |
| **Generation** | Pure AR | AR + Diffusion | AR + Diffusion |
| **ì•„í‚¤í…ì²˜** | Single unified | Two-stage | Two-stage |
| **RL ì§€ì›** | âœ… ê°€ëŠ¥ | âš ï¸ ì œí•œì  | âœ… ê°€ëŠ¥ |
| **Diffusion** | âŒ ì—†ìŒ | SDXL-Turbo | Sana (Flow Matching) |

---

### Trade-offs

#### ì¥ì 
1. **ë†’ì€ ì´ë¯¸ì§€ í’ˆì§ˆ**: Diffusion ëª¨ë¸ í™œìš©
2. **RL í˜¸í™˜ì„±**: Discrete tokensë¡œ policy gradient ê°€ëŠ¥
3. **ë‹¤ì–‘ì„±**: 256 tokens Ã— codebook sizeì˜ ì¡°í•©
4. **Multi-scale**: í•´ìƒë„ ì¡°ì ˆ ê°€ëŠ¥

#### ë‹¨ì 
1. **ì¶”ë¡  ì†ë„**: AR + 30 diffusion steps (ëŠë¦¼)
2. **Interleaved ì œì•½**: ìˆ˜ë™ìœ¼ë¡œ mode ì „í™˜ í•„ìš”
3. **ë©”ëª¨ë¦¬ ì‚¬ìš©**: AR model + Diffusion model + VAE
4. **ë³µì¡ì„±**: ë‘ ëª¨ë¸ ë™ì‹œ í•™ìŠµ í•„ìš”

---

### ìµœì¢… ìš”ì•½

**BLIP3o-NEXTëŠ”**:
- âŒ BOI í† í°ì„ ìë™ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨
- âŒ ì™„ì „ ìë™í™”ëœ interleaved generation ë¶ˆê°€
- âœ… í•˜ì§€ë§Œ discrete tokens + diffusionìœ¼ë¡œ ë†’ì€ ì´ë¯¸ì§€ í’ˆì§ˆ ë‹¬ì„±
- âœ… RL í”„ë ˆì„ì›Œí¬ë¡œ text-image alignment ê°œì„  ê°€ëŠ¥
- ğŸ¯ **Type B3 + Diffusion Hybrid**: Understandingê³¼ Generationì„ ê²°í•©í•œ ë…íŠ¹í•œ ì•„í‚¤í…ì²˜

**ì„¤ê³„ ì² í•™**:
- **Quality over Speed**: Diffusionìœ¼ë¡œ ìµœê³  í’ˆì§ˆ ì¶”êµ¬
- **RL-driven Alignment**: Discrete tokensë¡œ ê°•í™”í•™ìŠµ ê°€ëŠ¥
- **Modular Design**: ARê³¼ Diffusionì„ ë…ë¦½ì ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥
